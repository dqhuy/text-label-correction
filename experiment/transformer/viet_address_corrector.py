import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torch.utils.data import Dataset, DataLoader
import random

# ==== 1. Bá»™ dá»¯ liá»‡u máº«u cho huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­ ====

data = [
    # Äá»‹a chá»‰ viáº¿t táº¯t/lá»—i -> Ä‘Ãºng
    ("123 Dg Tráº§n HÆ°ng Äa", "123 ÄÆ°á»ng Tráº§n HÆ°ng Äáº¡o"),
    ("56 dg Phan ÄÃ¬nh PhÃ¹ng", "56 ÄÆ°á»ng Phan ÄÃ¬nh PhÃ¹ng"),
    ("09 ngo 2 PÄP", "09 NgÃµ 2 Phan ÄÃ¬nh PhÃ¹ng"),
    ("1 duong Nguyen Du", "1 ÄÆ°á»ng Nguyá»…n Du"),
    ("7 Dg Hai Ba Trung", "7 ÄÆ°á»ng Hai BÃ  TrÆ°ng"),
    ("sá»‘ 2 ngo 55 Le Duan", "sá»‘ 2 NgÃµ 55 LÃª Duáº©n"),
    ("14 ngo 3 KÄT Má»¹ DÃ¬nh", "14 NgÃµ 3 Khu Ä‘Ã´ thá»‹ Má»¹ ÄÃ¬nh"),
    ("25 duong 3/2", "25 ÄÆ°á»ng 3 ThÃ¡ng 2"),
]

random.shuffle(data)
train_data = data[:6]
val_data = data[6:]

# ==== 2. Khá»Ÿi táº¡o model vÃ  tokenizer ====

MODEL_NAME = "VietAI/vit5-base"  # Nháº¹ vÃ  tá»‘i Æ°u cho tiáº¿ng Viá»‡t
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# ==== 3. Dataset PyTorch ====

class AddressCorrectionDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        source, target = self.pairs[idx]
        input_text = f"Sá»­a Ä‘á»‹a chá»‰: {source}"
        input_ids = tokenizer(input_text, padding='max_length', truncation=True, max_length=64, return_tensors="pt").input_ids.squeeze()
        labels = tokenizer(target, padding='max_length', truncation=True, max_length=64, return_tensors="pt").input_ids.squeeze()
        labels[labels == tokenizer.pad_token_id] = -100
        return {"input_ids": input_ids, "labels": labels}

train_dataset = AddressCorrectionDataset(train_data)
val_dataset = AddressCorrectionDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=1)

# ==== 4. Huáº¥n luyá»‡n ====

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

print("Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")

for epoch in range(5):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)

        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    print(f"[Epoch {epoch+1}] Train loss: {total_loss/len(train_loader):.4f}")

# ==== 5. ÄÃ¡nh giÃ¡ vÃ  In káº¿t quáº£ ====

model.eval()
print("\n== Káº¿t quáº£ kiá»ƒm thá»­ ==")

for input_text, expected in val_data:
    full_input = f"Sá»­a Ä‘á»‹a chá»‰: {input_text}"
    input_ids = tokenizer(full_input, return_tensors="pt").input_ids.to(device)
    output_ids = model.generate(input_ids, max_length=64)
    predicted = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print(f"OCR: {input_text}")
    print(f"âœ” Ká»³ vá»ng: {expected}")
    print(f"ğŸ¤– Gá»£i Ã½:   {predicted}")
    print("-" * 40)
